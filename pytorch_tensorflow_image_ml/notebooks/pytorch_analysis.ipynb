{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Contents\n",
    "- Dataset loading\n",
    "- Model building\n",
    "    - Simple Linear\n",
    "    - Convolutional\n",
    "    - Complex\n",
    "- Model Output / Analysis\n",
    "\n",
    "## About\n",
    "We will walk through the simplest possible DNN model training implimentations.\n",
    "\n",
    "## Dataset\n",
    "The MNIST dataset is one of the most popular image datasets to work with. Each dataset has its one format / file structure, so do not be suprised if trying other dataset involves some fine tuning.\n",
    "\n",
    "You can use [Kaggle](https://www.kaggle.com/c/digit-recognizer/data) for downloading the data. Note that the training csv is the only file you can realistically train on, since the test set does not have labels.\n",
    "\n",
    "You can also go to the original dataset [here](http://yann.lecun.com/exdb/mnist/). Below, we will be using this dataset, however Kaggle is probably one of the greatest ways to get started in machine learning due to the ease of dataset finding and downloading.\n",
    "\n",
    "When you unzip it, you will find these:\n",
    "![imagetxt](../../res/mnist_files.png)\n",
    "\n",
    "Kind of weird. It's just some kind of ubyte file. Commonly most datasets have a csv format which is certainly easier. So lets do just that and make these csvs!\n",
    "\n",
    "In `pytorch_tensorflow_image_ml/utils` we have a file called `mnist_conversion.py`. This is a file created by others for handling this dataset. Not sure why the authors didn't just make the dataset as a csv, however I think the ubyte format makes it as small as possible, as well as allowing you to convert the entire dataset into any file format you would want. The folder should look like this:\n",
    "![imagetxt](../../res/mnist_with_converter.png)\n",
    "\n",
    "`cd` into the folder and call `python mnist_conversion.py` and we should get:\n",
    "![imagetxt](../../res/mnist_converter_result.png)\n",
    "\n",
    "Awesome! We have the csvs! But wait... Where are the images? First we need to structure the project a little. Typically our projects involve some structure like so:\n",
    "![imagetxt](../../res/project_data_structure_view.png)\n",
    "\n",
    "Some important notes, normally the data folder is in the the highest / 2nd higher directory so that any of the submodules can use it. Also, if you are using version control, having all the data in a single directory makes gitignoring it a lot easier. *Trying to push a large number of images / data into a VCS is, and has been, extremely messy.*\n",
    "\n",
    "Ok.\n",
    "\n",
    "We have th csvs, lets see what is in them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jlaivins/PycharmProjects/PyTorchTensorflowImageML/pytorch_tensorflow_image_ml/data/mnist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis is what the absolute path to the mnist directory looks like. You can either:\\nA. Manually make a full string to your mnist dir\\nB. Use relative pathing to the data dir ex: ../../data/mnist\\nC. Use my absolute pathing method (the best way obviously.)\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_tensorflow_image_ml.utils.file_handling import get_absolute_path\n",
    "absolute_path_to_mnist = get_absolute_path('mnist')\n",
    "print(absolute_path_to_mnist)\n",
    "\n",
    "\"\"\"\n",
    "This is what the absolute path to the mnist directory looks like. You can either:\n",
    "A. Manually make a full string to your mnist dir\n",
    "B. Use relative pathing to the data dir ex: ../../data/mnist\n",
    "C. Use my absolute pathing method (the best way obviously.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have some comments about pathing. Typically if you are just using a single computer, and a single user (you) **relative paths** are the easiest way. But if you try to deploy on a server, and it happens (for whatever reason) to get confused by the relative paths, then maybe use the server's **absolute paths**. *But what if you are using VCS? What if there are other users? Maybe you are working on a team? Maybe this needs to be deployed on different servers?* Then some automated way of getting the abolute path would be a good route. Also, keep in mind that different OS's handle paths differently, so using an automated path getter that can be robust to different OS's could be advantageous. If you want to make your own, go for it! I would use `import os` for pathing.\n",
    "\n",
    "We have the path to the folder now. Remember that the test set is not what we are going to train on, will show both. We are going to use the `pandas` library. This (in my opinion) is one of the most powerful libraries for doing anything with spreadsheet based data, and is one of the reasons python as a language has become so popular with the data science community (on top of the hundreds of other amazing python libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.608  0.609  0.610  \\\n",
      "0  0  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
      "1  4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
      "2  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
      "3  9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
      "4  2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
      "\n",
      "   0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
      "0      0      0      0      0      0      0      0  \n",
      "1      0      0      0      0      0      0      0  \n",
      "2      0      0      0      0      0      0      0  \n",
      "3      0      0      0      0      0      0      0  \n",
      "4      0      0      0      0      0      0      0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# The os.path.joint is better than using 'some_path' + '/' + 'filename' because it will determine the separator '/' \n",
    "# as needing to either be '/' or '\\'. This is a safer approach to combining paths.\n",
    "df = pd.read_csv(os.path.join(absolute_path_to_mnist, 'mnist_train.csv'), nrows=5)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Kind of. Not really. What the hell is this? Pandas primary data structure is called a *DataFrame*. This is basically a csv with functions. We have loaded basically a csv with 5 rows. There has to be a better way to digure out what is in this.\n",
    "\n",
    "I do not usually use Jupyter notebooks for a lot of my actual programs. I usually use [Pycharm](https://www.jetbrains.com/pycharm/), and if you run the above code in debug mode:\n",
    "![imagetxt](../../res/pycharm_eval_step_1.png)\n",
    "\n",
    "The big thing is the **Evaluation** button in the bottom middle (looks like a calculator)\n",
    "\n",
    "![imagetxt](../../res/pycharm_eval_step_2.png)\n",
    "![imagetxt](../../res/pycharm_eval_step_3.png)\n",
    "\n",
    "If you click *View as DataFrame* then the right side bar will have the DataFrame's contents for display! Keep in mind that you want to keep the DataFrame small because PyCharm starts having issues if the DataFrame is too large (takes a lot of memory). Hence above, we only read in 5 rows.\n",
    "\n",
    "![imagetxt](../../res/pycharm_eval_step_4.png)\n",
    "\n",
    "But notice that only the first column has non-zero values. This is important and you will see why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['5', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8',\n",
      "       ...\n",
      "       '0.608', '0.609', '0.610', '0.611', '0.612', '0.613', '0.614', '0.615',\n",
      "       '0.616', '0.617'],\n",
      "      dtype='object', length=785)\n",
      "There are 785 columns\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(f'There are {len(df.columns)} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so there is one columns labeled as **5** and the other columns increase in value from 0 to 0.617. \n",
    "This is super weird. \n",
    "Basically, the **5** column is where the **labels** are. So, what are the other columns for?\n",
    "The other columns? They are the literal pixels :o. Let's unpack this. Let do row 3 (as we discovered above.). \n",
    "Column '5' contains the labels, so the image at row 3 is an image of a number 9 apparently.\n",
    "\n",
    "**Our goal will be to have a model look at the image and predict that the image is a 9**\n",
    "Immediately, we are already interested in the values that we will be feeding. Column *5* will be our **y** and the other columns will be our **x** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5        9\n",
      "0        0\n",
      "0.1      0\n",
      "0.2      0\n",
      "0.3      0\n",
      "0.4      0\n",
      "0.5      0\n",
      "0.6      0\n",
      "0.7      0\n",
      "0.8      0\n",
      "0.9      0\n",
      "0.10     0\n",
      "0.11     0\n",
      "0.12     0\n",
      "0.13     0\n",
      "0.14     0\n",
      "0.15     0\n",
      "0.16     0\n",
      "0.17     0\n",
      "0.18     0\n",
      "0.19     0\n",
      "0.20     0\n",
      "0.21     0\n",
      "0.22     0\n",
      "0.23     0\n",
      "0.24     0\n",
      "0.25     0\n",
      "0.26     0\n",
      "0.27     0\n",
      "0.28     0\n",
      "        ..\n",
      "0.588    0\n",
      "0.589    0\n",
      "0.590    0\n",
      "0.591    0\n",
      "0.592    0\n",
      "0.593    0\n",
      "0.594    0\n",
      "0.595    0\n",
      "0.596    0\n",
      "0.597    0\n",
      "0.598    0\n",
      "0.599    0\n",
      "0.600    0\n",
      "0.601    0\n",
      "0.602    0\n",
      "0.603    0\n",
      "0.604    0\n",
      "0.605    0\n",
      "0.606    0\n",
      "0.607    0\n",
      "0.608    0\n",
      "0.609    0\n",
      "0.610    0\n",
      "0.611    0\n",
      "0.612    0\n",
      "0.613    0\n",
      "0.614    0\n",
      "0.615    0\n",
      "0.616    0\n",
      "0.617    0\n",
      "Name: 3, Length: 785, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "row_3 = df.loc[3]\n",
    "print(row_3)\n",
    "# So we have the row output. Notice that the first column (5) has a value of '9' which is our label. \n",
    "# Lets separent them.\n",
    "our_label = row_3[0]\n",
    "our_flat_image = row_3[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have reached a fork in the road. Are we doing a **continuous** problem or a **classification** problem?\n",
    "\n",
    "A **continuous problem** tries to predict continuous values such as predict tomorrow's tempurature, next week's stock price, the desired motor accelaration, the required steering angle. All of these are numbers that can range from a nice 0 &rarr; 1 (normalized values) such as probabilities, to -30 &rarr; 110 if tempuratures, to -180 &rarr; 180 for steering angles. \n",
    "\n",
    "A **classification problem** tries to predict categorical values such as predict is this a dog/cat/horse, is this a rock/pedestrian/road/stop sign, will this person buy this product (yes / no), is this yeast cell dead/alive. There can be 2 classes (yes / no), all the way up to 1000 classes (ImageNet).\n",
    "\n",
    "What are we trying to do? We are using numbers, so maybe this is continuous? **But wait**. We are trying to predict if an image is a 1, 2, 3, ... 9. There are a gauranteed 10 different numbers to predict! If this was a continuous problem, we would be trying to predict if an image is a 1, 1.1, 1.2, 1.3, 1.31... Like how tempurature might have many many numbers. Are we trying to do this? **No!** We are only trying to predict 10 numbers... **10 classes**. So this is a classification problem. \n",
    "\n",
    "So 1 sample of `y` should not be `(1, 1)` but `(1, 10 classes)`. And a set of `y` such as 5 samples should not be `(5, 1)` but `(5, 10 classes)`!\n",
    "\n",
    "### Important explaination: Normalization (OneHot)\n",
    "**Neural networks like _normalized_ data. Which means the data needs to be between 0 &rarr; 1.**\n",
    "No matter what, you will likely almost always do this / want to do this to your data. Both for your `x` and your `y`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9 is our label. So we have this. \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] ok... So where did the 9 go? \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] Awesome we still have no idea where 9 is... there's just a 1...\n",
      " 9 Oh... So onehot turns any class name into a range of 0 to 1\n",
      " We do this by adding as many zeros as there are classes, and putting a 1 at the index representing the label\n",
      " Then, np.argmax will return the index of the max value.\n",
      " Bascially... we hide the label as an index so that we can normalize the classes. We get it back via argmax!\n",
      " Our y is now normalized. If we were predicting numbers 0 to 999, we just change number_of_classes to 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# For a classification problem, we want to OneHot the `y`. So remember our `row_3` label?\n",
    "print(f' {our_label} is our label. So we have this. ')\n",
    "# To Onehot this, we make an empty version of the label\n",
    "number_of_classes = 10\n",
    "onehot_label = np.zeros((1, number_of_classes))  # For a shape (1, 10) 1 row, 10 features / columns\n",
    "print(f' {onehot_label} ok... So where did the 9 go? ')\n",
    "onehot_label[0, our_label] = 1  # Put the 1 at the place where the label is \n",
    "print(f' {onehot_label} Awesome we still have no idea where 9 is... there\\'s just a 1...')\n",
    "onehotted_label = np.argmax(onehot_label[0])  # max returns the value, argmax returns the location\n",
    "print(f' {onehotted_label} Oh... So onehot turns any class name into a range of 0 to 1')\n",
    "print(' We do this by adding as many zeros as there are classes, and putting a 1 at the index representing the label')\n",
    "print(' Then, np.argmax will return the index of the max value.')\n",
    "print(' Bascially... we hide the label as an index so that we can normalize the classes. We get it back via argmax!')\n",
    "print(' Our y is now normalized. If we were predicting numbers 0 to 999, we just change number_of_classes to 1000')\n",
    "normalized_image = our_flat_image / 255  # This is typically all you have to do to normalize images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above was kind of crazy. Just to help understanding normalizing data. If you wanted to predict the classes for dog and cat, then you might have a `y` like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['dog', 'cat', 'horse', 'cat', 'person', 'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh my god, `y` is a STRING?!?! How would we turn **these into numbers????**. This is common in image classification if we are getting our classes as strings from a file system. Easy, we just give these a code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0, 1, 2, 1, 3, 0] We have turned them into numbers! Just make sure that you remmember the original class_keys!\n",
      " All we would have to do is fix there shapes (right now they are as 1 row, should be 6 rows).\n",
      " then OneHot them to normalize their values\n"
     ]
    }
   ],
   "source": [
    "class_keys = {'dog':0, 'cat':1, 'horse':2, 'person':3}\n",
    "numeric_y = []\n",
    "for element in y:\n",
    "    numeric_y.append(class_keys[element])\n",
    "print(f' {numeric_y} We have turned them into numbers! Just make sure that you remmember the original class_keys!')\n",
    "print(' All we would have to do is fix there shapes (right now they are as 1 row, should be 6 rows).')\n",
    "print(' then OneHot them to normalize their values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The row shape (785,)\n",
      " The flat image shape (784,)\n",
      " The reshaped image shape (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So we separated the row into `our_label` and `our_flat_image`. Think of these are our `y` and our `x`\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "plt.title(f'The image number {onehotted_label}')\n",
    "numpy_array = np.array(normalized_image)\n",
    "plt.imshow(numpy_array.reshape(28, 28))\n",
    "our_flat_image = np.array(numpy_array)\n",
    "print(f' The row shape {row_3.shape}')\n",
    "print(f' The flat image shape {numpy_array.shape}')\n",
    "print(f' The reshaped image shape {numpy_array.reshape(28, 28).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! \n",
    "\n",
    "So we have a better understanding of the data we were working with. So to review what we have done:\n",
    "- We read **5** rows from a training csv using a very powerful csv library `pandas`\n",
    "- A single row has a shape of (785, )\n",
    "- The **y** label is in the first column index **0**. Removing it makes the row shape (784, )\n",
    "- As referenced in [MNIST](http://yann.lecun.com/exdb/mnist/) They say the image shape is *\"the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field\"*. The images are 28x28, and the remaining row has a **shape 784**, which means that you can sqrt(784) and get 28. This means we can **cleanly** reshape it into a 28x28 2d matrix.\n",
    "\n",
    "We have also made use of the `numpy` library! This is a library that uses Python / C bindings for extremely fast matrix calculations. This allows use to use the simplicity of Python with the power or C. In fact, you could create a decent neural net from scratch using only numpy. **Numpy like pandas, is a massive go-to library for data science / expensive math operations**\n",
    "\n",
    "\n",
    "#### Major Things to Realize\n",
    "A single image when turned into a numpy array is of shape (784, ). **This is a not row**. Neither is (785, )! You might get major dimension errors if you do not consider this. (784, ) currently is treated as \"785 rows\". In reality, we want it to be in the shape (1, 784) meaning \"1 row of 784 **features / columns**\". You might be a lot of issues if you do not understand this. and is resolved by doing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (784,) says \"784 rows\". Doesn't make sense, this is one row. Also known as a 1d matrix.\n",
      "\n",
      " The examples below you would think are 1d matrices because there is only 1 row, but that would be wrong. \n",
      " (1, 784) says \"1 row, 784 features / columns\". Better, '1' means 1 row, '-1' means dump everything else.\n",
      " (1, 784) says same as above. We basically shove an extra dimension at index '0'\n",
      " (1, 784) says the same also. We add a new axis to the start.\n"
     ]
    }
   ],
   "source": [
    "print(f' {numpy_array.shape} says \"784 rows\". Doesn\\'t make sense, this is one row. Also known as a 1d matrix.')\n",
    "\n",
    "print(f'\\n The examples below you would think are 1d matrices because there is only 1 row, but that would be wrong. ')\n",
    "print(f' {numpy_array.reshape(1, -1).shape} says \"1 row, 784 features / columns\". Better, \\'1\\' means 1 row, \\'-1\\' means dump everything else.')\n",
    "print(f' {np.expand_dims(numpy_array, 0).shape} says same as above. We basically shove an extra dimension at index \\'0\\'')\n",
    "print(f' {numpy_array[np.newaxis, :].shape} says the same also. We add a new axis to the start.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Continuation\n",
    "We have said a lot, but lets more formally set up our *x* and *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0 4 1 9 2]\n",
      " Our x shape (5, 784)\n",
      " Our y shape (5,)\n",
      " Fixed y shape (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets do this again but get super cerial this time.\n",
    "df = pd.read_csv(os.path.join(absolute_path_to_mnist, 'mnist_train.csv'), nrows=5)\n",
    "\n",
    "y = df['5'].values # Get the column '5', get the numpy array.\n",
    "print(f' {y}')\n",
    "# Get all the other columns (5 is in index 0, so we ignore it doing [1:] instead of just [:])\n",
    "remaining_columns = df.columns[1:] \n",
    "x = df[remaining_columns].values # Get just the numpy array\n",
    "print(f' Our x shape {x.shape}')\n",
    "print(f' Our y shape {y.shape}')\n",
    "y = y.reshape(-1, 1)\n",
    "print(f' Fixed y shape {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok final thing, If we were prediction a **continuous** value, then y only needs to have 1 feature.  Meaning that the final output is some continous values. **It is typically faster to train by doing a classification problem as opposed to a continuous problem.** The continuous problem is something like \"what is the temperature going to be tomorrow\", and classification problem is \"is this image a cat or a dog?\". *What are we trying to do?* We are trying to predict \"is this image a cat or a dog\", or \"is this image a 0, a 1, a 2 ...\". We absolutely could turn this into a continuous problem, but why would we? We have 10 known classes, so lets have a model that can do classification. So the y values will be converted into a (N, 10) shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (5, 10) is our new y samples. Each row is a sample, and there are 10 classes.\n",
      " (5, 1) currently has 5 samples, but 1 value. this is \"ok\" will be weirder to think about.\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      " This is a new y. 5 samples by 10 classes :)\n",
      "\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      " Now we have a revised y with 10 classes. The '1' is the \"correct\" class. \n",
      "\n",
      "\n",
      " We can get the original y values easily doing [0 4 1 9 2]\n",
      " Experiment with the axis part. In numpy the axis param provides different ways a function can be done on some matrix\n"
     ]
    }
   ],
   "source": [
    "class_y = np.zeros((y.shape[0], 10))\n",
    "print(f' {class_y.shape} is our new y samples. Each row is a sample, and there are 10 classes.')\n",
    "print(f' {y.shape} currently has 5 samples, but 1 value. this is \"ok\" will be weirder to think about.\\n')\n",
    "print(f' {class_y} \\n\\n This is a new y. 5 samples by 10 classes :)\\n\\n')\n",
    "class_y[np.arange(y.shape[0]), y[:, 0]] = 1\n",
    "print(f' {class_y} \\n\\n Now we have a revised y with 10 classes. The \\'1\\' is the \"correct\" class. \\n\\n')\n",
    "print(f' We can get the original y values easily doing {np.argmax(class_y, axis=1)}')\n",
    "print(' Experiment with the axis part. In numpy the axis param provides different ways a function can be done on some matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Y final shape: (5, 10) X final shape: (5, 784). Looking good!\n"
     ]
    }
   ],
   "source": [
    "# Y is normalized, lets do X (nothing changes, just as easy)\n",
    "y = class_y.copy()\n",
    "x = x / 255\n",
    "print(f' Y final shape: {y.shape} X final shape: {x.shape}. Looking good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Ok so as seen above, we have a pipeline of stripping data from a csv and making sure it is the correct shape.\n",
    "\n",
    "A safe data format is typically:\n",
    "\n",
    "*x.shape &rarr; (N Samples, F Features)*\n",
    "\n",
    "*y.shape &rarr; (N Samples, K Classes)*\n",
    "\n",
    "\n",
    "So as shown in the previous section, we got 5 rows from the csv. So:\n",
    "\n",
    "`N = 5`\n",
    "\n",
    "How many labels are we trying to predict? We are trying to predict whether an image is a 0, 1, 2, ... 9. In other words, there are 10 classes, 10 labels that we are trying t predict. So:\n",
    "\n",
    "`K = 10`\n",
    "\n",
    "How many features are we using to make this prediction? **All of the image pixels**. We are using all of the image pixels to predict a label. Kind of a \"duh\" thing to say. We are doing image classification, so you would think this is an obvious thing to say. The images are 28x28, so have a total of 784 pixels. So:\n",
    "\n",
    "`F = 784`\n",
    "\n",
    "So:\n",
    "\n",
    "x.shape is (5, 784)\n",
    "\n",
    "y.shape is (5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# Ref: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x.shape[1], 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, y.shape[1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! We have our neural net. Super easy with using the pytorch library (torch). It is great that we organized our samples above. This means that we want a model that does:\n",
    "\n",
    "x &rarr; &rarr; Linear Layer &rarr; Activation Function &rarr; Linear Layer &rarr; pred_y\n",
    "\n",
    "But what do we do with `y`? We will:\n",
    "1. Use `y` to show our model how *wrong* it is so it can improve, and... \n",
    "2. To get our model's accuracy. I mean, we want to know how good our model is right??\n",
    "\n",
    "We are using Linear Layers in this example. They are (in my opinion) the simplest elements in a neural net both in frameworks like this, and in custom from-scratch neural nets. \n",
    "\n",
    "**Linear Layer Qualities**\n",
    "\n",
    "Some Pros:\n",
    "- Simple\n",
    "- Each neuron in a layer connects to all the neurons in the next layer, thus can represent some pretty complex features\n",
    "\n",
    "Some (Big) Cons:\n",
    "- Each neuron in a layer connects to all the neurons in the next layer... This means they will be extremely slow to learn, and scale badly for large data samples. What if we feed it images 500x500? MNIST is only 28x28. Conceptually simple linear model can jump to gigabytes of weights / parameters if too many linear connections are being used. Also, some images have a lot of *useless* space such as a sky. We want our neural net to leverage common big features. For example, if we want to recognize faces we might be interested in eyes, mouths, ears. Lineary layers will put *equal importance* on every pixel..\n",
    "\n",
    "This is why it is normally common to have a single linear layer at the end of a complex model with other layers. However, MNIST images are small, so we can get away will using Linear Layers here.\n",
    "\n",
    "**Activation Functions**\n",
    "\n",
    "There are many activation functions. These decide when a single neuron \"fires\". Here are a few:\n",
    "- ReLU\n",
    "- LeakyReLU\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- Softmax\n",
    "\n",
    "There are many more [Activation Functions](https://en.wikipedia.org/wiki/Activation_function).\n",
    "\n",
    "**Neural Net Description**\n",
    "\n",
    "We are using the pytorch `Sequential Module` for structuring our Neural Net. We certainly do not have to use it to make the same type of model, but this is much simpler. A simple and common neural net involves one layer passing its activations to next layer then the next then the next till it gets to the last layer.\n",
    "\n",
    "## ADD A FIGURE HERE SHOWING THE NEURAL NET STACK\n",
    "\n",
    "Remember that `x.shape` is `(N, 784)` and so `x.shape[1]` is 784. So `Linear(x.shape[1], 64)` says we want to feed in samples with 784 features, and output 64 activations. **Now why did we choose 64?** Well... just because? Hidden layer sizes are usually chosen based on how well certain sizes affect the model accuracy / training time. The only times that hidden layer sizes matter typically is when using more complex layers such as convolutional neural nets. \n",
    "\n",
    "Our Neural Net is pretty simple, so after `Linear(x.shape[1], 64)` and the activation, we get to our output layer `Linear(64, y.shape[1])`. So `Linear(x.shape[1], 64)` and `Linear(64, y.shape[1])` need to be connected, the the first's output needs to be the same as the next's input. `Linear(64, y.shape[1])` is our output layer, so its output is the number of features of `y` which is the number of classes we are predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our loss function. This will decide how wrong the model's predictions are. To be more technical, \n",
    "# it is how much the gradients are changing. If the gradients flatten out, \n",
    "# it is not / does not need to learn anymore.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "epochs = 500\n",
    "learning_rate = 1e-4  # Same as 0.0001\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_tensor = torch.tensor(y).float()\n",
    "    x_tensor = torch.tensor(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0412186668418144e-07\n",
      "1 1.0372296088689836e-07\n",
      "2 1.0338262512732399e-07\n",
      "3 1.0303284625479137e-07\n",
      "4 1.0275296347117546e-07\n",
      "5 1.0244915671364652e-07\n",
      "6 1.0212323786618072e-07\n",
      "7 1.0180671239368166e-07\n",
      "8 1.0149955187443993e-07\n",
      "9 1.0121559057552076e-07\n",
      "10 1.0087935464753173e-07\n",
      "11 1.0060006871981386e-07\n",
      "12 1.0030102970404187e-07\n",
      "13 1.0000371020169041e-07\n",
      "14 9.96996973867681e-08\n",
      "15 9.938323586311526e-08\n",
      "16 9.910692710946023e-08\n",
      "17 9.881750173690307e-08\n",
      "18 9.85003580922239e-08\n",
      "19 9.821129509646198e-08\n",
      "20 9.7922509212367e-08\n",
      "21 9.761899377735972e-08\n",
      "22 9.732066530432348e-08\n",
      "23 9.70598676985901e-08\n",
      "24 9.6773682400908e-08\n",
      "25 9.645419396520083e-08\n",
      "26 9.617249219218138e-08\n",
      "27 9.594199923412816e-08\n",
      "28 9.564404024331452e-08\n",
      "29 9.535248324255008e-08\n",
      "30 9.505891540584344e-08\n",
      "31 9.480516638404879e-08\n",
      "32 9.450099014429725e-08\n",
      "33 9.423175129086303e-08\n",
      "34 9.393045985461868e-08\n",
      "35 9.367266784465755e-08\n",
      "36 9.340305240357338e-08\n",
      "37 9.311903426123536e-08\n",
      "38 9.284967461553606e-08\n",
      "39 9.258587141403041e-08\n",
      "40 9.231150954747136e-08\n",
      "41 9.203012041325564e-08\n",
      "42 9.177762194667594e-08\n",
      "43 9.149979973699374e-08\n",
      "44 9.123369437702422e-08\n",
      "45 9.097514208633584e-08\n",
      "46 9.0706308242261e-08\n",
      "47 9.043194637570195e-08\n",
      "48 9.016433466513263e-08\n",
      "49 8.993448830096895e-08\n",
      "50 8.96546978879087e-08\n",
      "51 8.940781270894149e-08\n",
      "52 8.91631373178825e-08\n",
      "53 8.889150393542877e-08\n",
      "54 8.862537015374983e-08\n",
      "55 8.836373410758824e-08\n",
      "56 8.811609575332113e-08\n",
      "57 8.784983407394975e-08\n",
      "58 8.763374381715039e-08\n",
      "59 8.73667289624791e-08\n",
      "60 8.71131220492316e-08\n",
      "61 8.686521368872491e-08\n",
      "62 8.66144915789846e-08\n",
      "63 8.636104809056633e-08\n",
      "64 8.611798563151751e-08\n",
      "65 8.586213340322502e-08\n",
      "66 8.56306172636323e-08\n",
      "67 8.54131272376435e-08\n",
      "68 8.514157912031806e-08\n",
      "69 8.488427027941725e-08\n",
      "70 8.465365652909895e-08\n",
      "71 8.441012511184454e-08\n",
      "72 8.415204177936175e-08\n",
      "73 8.392434125426007e-08\n",
      "74 8.370766835241739e-08\n",
      "75 8.34490663237375e-08\n",
      "76 8.320670730199708e-08\n",
      "77 8.298039233523014e-08\n",
      "78 8.274121654494593e-08\n",
      "79 8.248909466601617e-08\n",
      "80 8.224529324252217e-08\n",
      "81 8.202338364071693e-08\n",
      "82 8.179021904197725e-08\n",
      "83 8.154259489856486e-08\n",
      "84 8.130134432349223e-08\n",
      "85 8.106860605039401e-08\n",
      "86 8.085045521966094e-08\n",
      "87 8.059588907372017e-08\n",
      "88 8.036438714498217e-08\n",
      "89 8.01516009119041e-08\n",
      "90 7.991581441046947e-08\n",
      "91 7.967197035441131e-08\n",
      "92 7.944284163841075e-08\n",
      "93 7.921592981574577e-08\n",
      "94 7.898663767491598e-08\n",
      "95 7.87637475241354e-08\n",
      "96 7.851460992469583e-08\n",
      "97 7.829188319874447e-08\n",
      "98 7.805681434547296e-08\n",
      "99 7.786342592908113e-08\n",
      "100 7.76304389660254e-08\n",
      "101 7.740432295122446e-08\n",
      "102 7.719015115981165e-08\n",
      "103 7.696335302398438e-08\n",
      "104 7.67176047133944e-08\n",
      "105 7.650234579159587e-08\n",
      "106 7.629336806758147e-08\n",
      "107 7.607055607650182e-08\n",
      "108 7.584916517089368e-08\n",
      "109 7.563161119605866e-08\n",
      "110 7.541007107647602e-08\n",
      "111 7.517777333987397e-08\n",
      "112 7.497311571569298e-08\n",
      "113 7.479565056200954e-08\n",
      "114 7.456240069814157e-08\n",
      "115 7.433650495158872e-08\n",
      "116 7.413662217459205e-08\n",
      "117 7.392954870510948e-08\n",
      "118 7.371071575335009e-08\n",
      "119 7.34807201752119e-08\n",
      "120 7.329119000587525e-08\n",
      "121 7.307032490189158e-08\n",
      "122 7.28700086938261e-08\n",
      "123 7.266420709584054e-08\n",
      "124 7.247510325214535e-08\n",
      "125 7.22590272062007e-08\n",
      "126 7.204076268862991e-08\n",
      "127 7.183826511436564e-08\n",
      "128 7.164216242472321e-08\n",
      "129 7.142410396454579e-08\n",
      "130 7.122746126242419e-08\n",
      "131 7.101942856024834e-08\n",
      "132 7.083642117322597e-08\n",
      "133 7.063283646857599e-08\n",
      "134 7.043151128982572e-08\n",
      "135 7.023613335377377e-08\n",
      "136 7.004246072028764e-08\n",
      "137 6.985576561646667e-08\n",
      "138 6.964013721244555e-08\n",
      "139 6.943991337493571e-08\n",
      "140 6.924036455302485e-08\n",
      "141 6.90404604597461e-08\n",
      "142 6.8849431045237e-08\n",
      "143 6.863820800617759e-08\n",
      "144 6.845222344509239e-08\n",
      "145 6.82564476051084e-08\n",
      "146 6.807617580761871e-08\n",
      "147 6.787720252532381e-08\n",
      "148 6.769409566231843e-08\n",
      "149 6.749890246737777e-08\n",
      "150 6.730081025807522e-08\n",
      "151 6.71004158903088e-08\n",
      "152 6.692117437978595e-08\n",
      "153 6.673032260096079e-08\n",
      "154 6.653450412841266e-08\n",
      "155 6.634545002270897e-08\n",
      "156 6.616820513727362e-08\n",
      "157 6.598484247888337e-08\n",
      "158 6.577592870371518e-08\n",
      "159 6.56009007116154e-08\n",
      "160 6.54278693446031e-08\n",
      "161 6.521477757814864e-08\n",
      "162 6.502470029090546e-08\n",
      "163 6.485898040864413e-08\n",
      "164 6.466782309644259e-08\n",
      "165 6.448467360087307e-08\n",
      "166 6.430281018765527e-08\n",
      "167 6.411734432276717e-08\n",
      "168 6.392730966808813e-08\n",
      "169 6.376716754630252e-08\n",
      "170 6.359048398962841e-08\n",
      "171 6.339630687079989e-08\n",
      "172 6.322014201032289e-08\n",
      "173 6.302663280166598e-08\n",
      "174 6.286764886453966e-08\n",
      "175 6.26774507850314e-08\n",
      "176 6.249755557519165e-08\n",
      "177 6.234137828187158e-08\n",
      "178 6.216306758233259e-08\n",
      "179 6.196329849217364e-08\n",
      "180 6.180261635790885e-08\n",
      "181 6.162277088606061e-08\n",
      "182 6.143503128441807e-08\n",
      "183 6.128124141469016e-08\n",
      "184 6.111047667900493e-08\n",
      "185 6.092803062074381e-08\n",
      "186 6.075865854882068e-08\n",
      "187 6.059362789301304e-08\n",
      "188 6.041005207180206e-08\n",
      "189 6.023940812838191e-08\n",
      "190 6.006165875760416e-08\n",
      "191 5.99068386009094e-08\n",
      "192 5.972632521888954e-08\n",
      "193 5.9558583842544977e-08\n",
      "194 5.9387893713847006e-08\n",
      "195 5.9212055703028454e-08\n",
      "196 5.905711120135493e-08\n",
      "197 5.888055198965958e-08\n",
      "198 5.871557107184344e-08\n",
      "199 5.8567202643189376e-08\n",
      "200 5.8386678603028486e-08\n",
      "201 5.820673010248356e-08\n",
      "202 5.8048946982580674e-08\n",
      "203 5.788487200675263e-08\n",
      "204 5.7727582714051096e-08\n",
      "205 5.7562640876085425e-08\n",
      "206 5.7392064434225176e-08\n",
      "207 5.7220638893795694e-08\n",
      "208 5.705703998160061e-08\n",
      "209 5.690613491537988e-08\n",
      "210 5.6740965703738766e-08\n",
      "211 5.658287349774582e-08\n",
      "212 5.6419612093350224e-08\n",
      "213 5.625109267271e-08\n",
      "214 5.609664199823783e-08\n",
      "215 5.594145946474782e-08\n",
      "216 5.579245154763157e-08\n",
      "217 5.563298444144493e-08\n",
      "218 5.546437265024906e-08\n",
      "219 5.529762958644824e-08\n",
      "220 5.515554946100565e-08\n",
      "221 5.500342226127941e-08\n",
      "222 5.4836721830042734e-08\n",
      "223 5.468079322668018e-08\n",
      "224 5.452761442370502e-08\n",
      "225 5.436464789454476e-08\n",
      "226 5.4222127232606e-08\n",
      "227 5.406195313639728e-08\n",
      "228 5.390070612065756e-08\n",
      "229 5.3751442408156436e-08\n",
      "230 5.3597698723706344e-08\n",
      "231 5.342939246588685e-08\n",
      "232 5.3294964885708396e-08\n",
      "233 5.314840834103052e-08\n",
      "234 5.299758143451072e-08\n",
      "235 5.2836000463685195e-08\n",
      "236 5.2685692253362504e-08\n",
      "237 5.255362722778045e-08\n",
      "238 5.240519129756649e-08\n",
      "239 5.2231726499485376e-08\n",
      "240 5.208981335158569e-08\n",
      "241 5.195077790176583e-08\n",
      "242 5.179389361842368e-08\n",
      "243 5.164413963143488e-08\n",
      "244 5.150171134005177e-08\n",
      "245 5.13518187972295e-08\n",
      "246 5.1203947748490464e-08\n",
      "247 5.1055057070925614e-08\n",
      "248 5.092024579766985e-08\n",
      "249 5.0768026227387963e-08\n",
      "250 5.061923502580612e-08\n",
      "251 5.048101314741871e-08\n",
      "252 5.034707584172793e-08\n",
      "253 5.0187470179707816e-08\n",
      "254 5.005725256523874e-08\n",
      "255 4.9929028023143474e-08\n",
      "256 4.978248213660663e-08\n",
      "257 4.9634621746008634e-08\n",
      "258 4.9496922116532005e-08\n",
      "259 4.9371490007388275e-08\n",
      "260 4.9210310493208453e-08\n",
      "261 4.907922246388807e-08\n",
      "262 4.894315708270369e-08\n",
      "263 4.881124127109615e-08\n",
      "264 4.8662457174941665e-08\n",
      "265 4.8533497221114885e-08\n",
      "266 4.840208944756341e-08\n",
      "267 4.8261007634664566e-08\n",
      "268 4.811573717233841e-08\n",
      "269 4.797511721221781e-08\n",
      "270 4.7853461637714645e-08\n",
      "271 4.773065498397955e-08\n",
      "272 4.759020555411553e-08\n",
      "273 4.746897630525382e-08\n",
      "274 4.7323027274615015e-08\n",
      "275 4.7188677854137495e-08\n",
      "276 4.707187528651957e-08\n",
      "277 4.69295571292605e-08\n",
      "278 4.679805698515338e-08\n",
      "279 4.666637565264864e-08\n",
      "280 4.6548361609666244e-08\n",
      "281 4.641135475935698e-08\n",
      "282 4.627505134635612e-08\n",
      "283 4.615786508566089e-08\n",
      "284 4.602499359407375e-08\n",
      "285 4.5904840817456716e-08\n",
      "286 4.577506729219749e-08\n",
      "287 4.5649979796280604e-08\n",
      "288 4.55044499858559e-08\n",
      "289 4.538632936146314e-08\n",
      "290 4.5275069027184145e-08\n",
      "291 4.515823448514311e-08\n",
      "292 4.499960937209835e-08\n",
      "293 4.489181293365618e-08\n",
      "294 4.4761101491985755e-08\n",
      "295 4.4637292972993237e-08\n",
      "296 4.4512130870089095e-08\n",
      "297 4.440332901367583e-08\n",
      "298 4.426951250025013e-08\n",
      "299 4.41578897891759e-08\n",
      "300 4.4035953550292106e-08\n",
      "301 4.3931045468070806e-08\n",
      "302 4.381062623792786e-08\n",
      "303 4.36860538854944e-08\n",
      "304 4.3571812824438894e-08\n",
      "305 4.345641002601042e-08\n",
      "306 4.332877168167215e-08\n",
      "307 4.3218403078526535e-08\n",
      "308 4.3122202697531975e-08\n",
      "309 4.29817461622406e-08\n",
      "310 4.2873139705079666e-08\n",
      "311 4.2762401619711454e-08\n",
      "312 4.2646917108868365e-08\n",
      "313 4.253476859616967e-08\n",
      "314 4.24133652643377e-08\n",
      "315 4.2305654091023825e-08\n",
      "316 4.218435378788854e-08\n",
      "317 4.207176829140735e-08\n",
      "318 4.196638414555309e-08\n",
      "319 4.185215018992494e-08\n",
      "320 4.172837009264185e-08\n",
      "321 4.162451716638316e-08\n",
      "322 4.151332078095038e-08\n",
      "323 4.1400316064255094e-08\n",
      "324 4.1296821962077956e-08\n",
      "325 4.117920582302759e-08\n",
      "326 4.1068176415137714e-08\n",
      "327 4.0963058722809365e-08\n",
      "328 4.0853059601886343e-08\n",
      "329 4.0734583706125704e-08\n",
      "330 4.0616765062395643e-08\n",
      "331 4.051649682423886e-08\n",
      "332 4.0418676405806764e-08\n",
      "333 4.0300495385281465e-08\n",
      "334 4.0223220310053875e-08\n",
      "335 4.0100982090507387e-08\n",
      "336 3.9987309463640486e-08\n",
      "337 3.98828845504795e-08\n",
      "338 3.978354712330656e-08\n",
      "339 3.966896500173789e-08\n",
      "340 3.9561758313766404e-08\n",
      "341 3.946121296394267e-08\n",
      "342 3.9348535096905835e-08\n",
      "343 3.925057967535395e-08\n",
      "344 3.913462265359158e-08\n",
      "345 3.903993217591051e-08\n",
      "346 3.8936544655143734e-08\n",
      "347 3.883141985738803e-08\n",
      "348 3.8731972296091044e-08\n",
      "349 3.8631210230732904e-08\n",
      "350 3.8527044665670473e-08\n",
      "351 3.842786000518572e-08\n",
      "352 3.8315093320306914e-08\n",
      "353 3.8225035581263e-08\n",
      "354 3.8112325739803055e-08\n",
      "355 3.802944092967664e-08\n",
      "356 3.790738745124145e-08\n",
      "357 3.7802578845003154e-08\n",
      "358 3.7726504586999e-08\n",
      "359 3.762099964887966e-08\n",
      "360 3.749938315422696e-08\n",
      "361 3.7409481734584915e-08\n",
      "362 3.732307618520281e-08\n",
      "363 3.7224104687538784e-08\n",
      "364 3.7110364559111986e-08\n",
      "365 3.7018963894297485e-08\n",
      "366 3.69226000884737e-08\n",
      "367 3.682380267377994e-08\n",
      "368 3.6720617657692856e-08\n",
      "369 3.663279102283923e-08\n",
      "370 3.653563140915139e-08\n",
      "371 3.6435540806678546e-08\n",
      "372 3.634368894722684e-08\n",
      "373 3.6242575163214497e-08\n",
      "374 3.6148101401067834e-08\n",
      "375 3.6054387919648434e-08\n",
      "376 3.596596798161045e-08\n",
      "377 3.587527075410435e-08\n",
      "378 3.577753560080055e-08\n",
      "379 3.5681104293416865e-08\n",
      "380 3.558536931791423e-08\n",
      "381 3.5497237149684224e-08\n",
      "382 3.540206350294284e-08\n",
      "383 3.530872660917339e-08\n",
      "384 3.521465430367243e-08\n",
      "385 3.511810220402367e-08\n",
      "386 3.503986079067545e-08\n",
      "387 3.493985900604457e-08\n",
      "388 3.4840830664961686e-08\n",
      "389 3.4758699030135176e-08\n",
      "390 3.4662647863115126e-08\n",
      "391 3.456996822137626e-08\n",
      "392 3.4486781430587143e-08\n",
      "393 3.439838636154491e-08\n",
      "394 3.429164152635167e-08\n",
      "395 3.420267091769347e-08\n",
      "396 3.412112192791028e-08\n",
      "397 3.403448189942537e-08\n",
      "398 3.395854974996837e-08\n",
      "399 3.3854430370183763e-08\n",
      "400 3.376981894120945e-08\n",
      "401 3.368183243424028e-08\n",
      "402 3.358517375318115e-08\n",
      "403 3.350076838160021e-08\n",
      "404 3.3403217969407706e-08\n",
      "405 3.33352296877365e-08\n",
      "406 3.323640029861963e-08\n",
      "407 3.3149898825968194e-08\n",
      "408 3.30640439472063e-08\n",
      "409 3.297776629551663e-08\n",
      "410 3.288663208422804e-08\n",
      "411 3.280263882743384e-08\n",
      "412 3.271389559245108e-08\n",
      "413 3.263754777549366e-08\n",
      "414 3.255754776887443e-08\n",
      "415 3.246176660809397e-08\n",
      "416 3.239132695398439e-08\n",
      "417 3.2299379171263354e-08\n",
      "418 3.220584687824157e-08\n",
      "419 3.213276045244129e-08\n",
      "420 3.2050518683490736e-08\n",
      "421 3.196796427573645e-08\n",
      "422 3.187347630273507e-08\n",
      "423 3.178585927798849e-08\n",
      "424 3.171209783658924e-08\n",
      "425 3.1623244467482436e-08\n",
      "426 3.154874050892431e-08\n",
      "427 3.145606441989912e-08\n",
      "428 3.1378004194948517e-08\n",
      "429 3.129641967802854e-08\n",
      "430 3.1204475448021185e-08\n",
      "431 3.112786117753785e-08\n",
      "432 3.105441237494233e-08\n",
      "433 3.0962549857349586e-08\n",
      "434 3.0884788060348e-08\n",
      "435 3.0805519912746604e-08\n",
      "436 3.0730078037777275e-08\n",
      "437 3.065333231688783e-08\n",
      "438 3.0573222176144554e-08\n",
      "439 3.047705732228678e-08\n",
      "440 3.039976803620448e-08\n",
      "441 3.031476225601182e-08\n",
      "442 3.024430128562017e-08\n",
      "443 3.0152556007578823e-08\n",
      "444 3.0091953817645845e-08\n",
      "445 3.000543102871234e-08\n",
      "446 2.992969427850767e-08\n",
      "447 2.9840428794614127e-08\n",
      "448 2.9764942510723813e-08\n",
      "449 2.968663714852937e-08\n",
      "450 2.9595639716717415e-08\n",
      "451 2.9534882983739408e-08\n",
      "452 2.9449736871356436e-08\n",
      "453 2.9367363651999767e-08\n",
      "454 2.9299245696279286e-08\n",
      "455 2.9220112551797683e-08\n",
      "456 2.9148759850272654e-08\n",
      "457 2.906228324661697e-08\n",
      "458 2.8993792255960216e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459 2.890044825676341e-08\n",
      "460 2.883751548665714e-08\n",
      "461 2.875395743728859e-08\n",
      "462 2.8681416353038003e-08\n",
      "463 2.8616256031455123e-08\n",
      "464 2.8545748875785648e-08\n",
      "465 2.846778812681805e-08\n",
      "466 2.8393587925279462e-08\n",
      "467 2.8312639344107993e-08\n",
      "468 2.8241229799164103e-08\n",
      "469 2.8166402543661206e-08\n",
      "470 2.8086198256005446e-08\n",
      "471 2.8024496501188878e-08\n",
      "472 2.7951386982749682e-08\n",
      "473 2.7884267339572943e-08\n",
      "474 2.779184349321895e-08\n",
      "475 2.7725443274562167e-08\n",
      "476 2.7665002733101574e-08\n",
      "477 2.7590663975729512e-08\n",
      "478 2.7515243417042257e-08\n",
      "479 2.745386140645678e-08\n",
      "480 2.7380432143786493e-08\n",
      "481 2.7308715289109387e-08\n",
      "482 2.7248033163118635e-08\n",
      "483 2.7172657013352364e-08\n",
      "484 2.7096303867324423e-08\n",
      "485 2.7028409732565706e-08\n",
      "486 2.6959570575968428e-08\n",
      "487 2.6886805670756075e-08\n",
      "488 2.682400612741276e-08\n",
      "489 2.6756163507002384e-08\n",
      "490 2.6677692943621878e-08\n",
      "491 2.6606379321947315e-08\n",
      "492 2.6537220421118946e-08\n",
      "493 2.6466750568943098e-08\n",
      "494 2.6413781384349022e-08\n",
      "495 2.6344187276095e-08\n",
      "496 2.627271733501857e-08\n",
      "497 2.620282124610185e-08\n",
      "498 2.614026151093185e-08\n",
      "499 2.6074284065202846e-08\n"
     ]
    }
   ],
   "source": [
    "# Explainations are from https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html\n",
    "for t in range(epochs):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x_tensor)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
